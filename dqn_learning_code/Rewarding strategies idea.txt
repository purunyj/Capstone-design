---------------- Rewarding Strategies ----------------

Write down any ideas about rewarding strategies to improve the performance.

1. 
공의 표현이 이산적이지 않고, 많은 state를 반복하여도 손해가 없기 때문에 deadlock이 걸리는 경우가 있다.
앞으로 같은 state들을 반복하면 음의 reward를 주면 해결 될 것 같다.

Since the position of the ball is not discrete (I not sure whether I understand correctly - Bomi), and there is no reduction of the point even if it repeat the state, there are occasions fall in to deadlock.
This might be fixed if negative reward is given for the repeating state.

## 같은 자리에서 반복하는 경우가 다반사인데
 rx ry를 받아와서, 몇 초동안 비슷한 위치면 감점하는 식이나 현재 10번의 step동안 공이 안들어오면 꺼지게 되어있는데 그냥 감점하는 식으로 하면 되지 않을까요.. 


2. 
지금도 원하는 sorting plate state로 공을 줍지만, 후에 원하는 만큼 교육이 되지 않는다면 rb(ball color) 2 channel 혹은, g(obstacle) 3 channel로 교육을 시도해봐야겠다.
지금은 비슷한 수 (gray_color = {"red_ball":255, "blue_ball":220, "wall":100, "robot":200, "robot_padding":150})를 이용하여 만든 이미지로 러닝하므로, 효과적이지 못 할 수 있다.

So far, sorting plate seems work well but if it cannot be improved over certain level, we might use 2 channel - RB (ball color) or 3 channel - RGB (including obstacle) for learning.
At this moment, we use (gray_color = {"red_ball":255, "blue_ball":220, "wall":100, "robot":200, "robot_padding":150}), which might not be clearly distinguishable from gray image, and thus become not effective.


3.
교육된 모델의 경로가 약간 진동하는 경우도 있어서, 같은 action을 반복하면 약간의 reward를 줄 생각이었다.
그러나 대각선 action수를 줄이는게 더 효과적으로 보인다.
이로도 부족하면 추가하는 것으로 생각하고 있다.

There are some cases where the path of the model oscillate and thus small amount of reward is one of the solution for the same action.
However, reducing the diagonal direction action seems more effective.
If these do not improve the performance, maybe add some actions. 

4. 
11/20 변경사항-resol 2 조절, steps 149로 조절, 공 간격 최소 10(100cm), 같은 자리에 있을 때 -0.05점, 안보일때 회전하는거 +0.001점 ==> not good performance
11/21 공 간격 최소 5(5cm), 같은 자리에 있을 때 -0.5점, 안보일때 회전하는거 +0.01점 ==> 마지막 공안 줍는거랑, 같은자리에서 감점당하는 건 똑같음
11/22- 공 간격 최소 5(5cm), 같은 자리에 있을 때 -0.5점, 안보일때 회전하는거 +0.05점, action 종류 :: forward 0,  right 1, left 2, cw 3 , ccw 4, red forward 5, blue forward 6 총 7까지 action으로 만듬 
       - 혹시 몰라 desktop 폴더 uuuuu안에 있음
11/23 계획 -dqn learning test2.py에 만들었다.
-1. 교육을 시작할때 공 10개라는 고정된 갯수로 사용하여 교육한다.-- 볼 사이즈 간격을 최대한 줄임
 --교육의 효율을 높이기 위해서, 현재 교육시키는 모델 에서 공의 갯수가 다른데 이는 다 먹어도, 목표 점수가 다르기 때문에 교육에 있어서 조금 악영향을 줄 수 있다. 
-2. 볼 6개를 먹었을때 (self.six_ball_collect=1) global 변수를 바꾸어 토픽 메세지로 보내준다. --실제시스템에서 볼을 6개 먹었을 때 중간에 끊어버리기 위함. ==> ((이후 고려, 차피 메세지만 추가하면 됨))
-3. 마지막 공 즉 10개를 다 먹었을 때, 추가점수를 부여한다. 이 때 추가점수는 지금의 step 수를 읽어와서 (149-step)/2 만큼 줄 예정. 
-4. 공을 다 먹어도 종료하지 않는다. 공평하게 149 스텝을 다 채웠을때의 점수를 비교할 것임. 또 마지막 공을 먹었을 때 점수가 추가되는데 뭔가 영향을 적게 받아서 다 먹어도 여유값을 줄 예정. ((이후 고려, 안할듯))
-5. replay buffer 수를 200000으로 늘려볼껄 고민 중.((이후 고려))
<구체적 실행>
++ball margin 2.5로함
++ 안보일때 회전하는거 +0.3 점  공 간격 최소 5(5cm), 같은 자리에 있을 때 -0.2점 왜냐! 공이 안보일때 이동 회전 이동 회전 이동 회전 이라도 하면서 공을 찾으려는 노력이 필요해 보임. 0.3점 
이때 만약 회전만 한다면? 0.1점이다 공이 안보일때는 어쨋든 회전 및 이동하게 하면서 점수를 얻게 하려는 목적. 또 이를 악용하지 않도록 하기 위해 최대한 점수를 조정하겠음
++공을 다먹었을 땐 어마어마한 점수를 줌 !! 다먹었을 때 지금까지의 스텝수를 max_iter에서 뺀 값이라는 큰 점수를 줌!! 
